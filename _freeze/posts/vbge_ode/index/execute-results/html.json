{
  "hash": "74fbe83af9c6c06cd370ec2add6cf88e",
  "result": {
    "markdown": "---\ntitle: \"Fitting a Pütter/von Bertalanffy ODE in brms\"\nauthor: \"Max Lindmark\"\ndate: \"2023-02-15\"\ncategories: [R, Bayesian, stats, ode, brms]\nimage: \"image.jpg\"\nbibliography: references.bib\n---\n\n\nThe Pütter growth model [@puetterStudienUeberPhysiologische1920] states that growth is the result of tissue synthesis (*\"anabolism\"*) and tissue breakdown (\"*catabolism*\"), expressed in the formula:\n\n$$dW/dt=aW^y-bW^z$$ In the 1930's, Ludwig von Bertalanffy published a series of papers (1932, 1934 and 1938. 1938 also happens to be the year [he joined the Nazi party](https://en.wikipedia.org/wiki/Ludwig_von_Bertalanffy)). In these, he introduced assumptions based on geometry that much simplified the solution of the Pütter model. Specifically, $z=1$[^1]. This allowed him to easily recast the Pütter ODE in terms of weight as a function of time (this is the version in [-@essingtonBertalanffyGrowthFunction2001]):\n\n[^1]: In Pauly's words this makes sense *\"because it \\[catabolism\\] consists of the spontaneous de-naturation of the proteins and other molecules contributing to that weight\"* [@paulyGilloxygenLimitationTheory2021]. Pauly's GOLT is another attempt to justify assumptions in the von Bertalanffy model and to argue for a mechanistic basis of it. I won't go in on that here, but I provide references below to some interesting comments and reply papers published *Trends in Ecology and Evolution* and *Global Change Biology* between 2017 and 2019.\n\n$$W_t=W_\\infty(1-\\exp(-K_{sp}(t-t_0)))^3$$ In other words, the von Bertalanffy equation (VBGE) is a special case of the Pütter model, where catabolism is proportional to mass.\n\nAn even more special version of this model is the **specialized** von Bertalanffy equation [@bevertonDynamicsExploitedFish1957; @paulyRelationshipsGillSurface1981; @ursinMathematicalModelAspects1967]. By also assuming that $W\\propto L^3$ and that $b=2/3$, you can cast this as a model of length (which in easier to measure in a fish while on a boat):\n\n$$L_t=L_\\infty(1-\\exp(-K(t-t_0)))$$ And this is the version we all love and most fisheries biologists are familiar with. It's the most common model for describing length as a function of age, very likely because it was adopted by Beverton and Holt [-@bevertonDynamicsExploitedFish1957]. von Bertalanffy [-@vonbertalanffyLawsMetabolismGrowth1957] writes:\n\n\"*It appears that the \"Bertalanffy growth equation\" is widely applied in international fisheries. It has been found to fit the commercially exploited fish species studied by the Fisheries Laboratory of the Ministry of Agriculture, Fisheries and Food at Lowestoft (England), with the possible exception of the hake (Wimpenny, pers. commun.)*\"[^2]\n\n[^2]: I'm sure someone has since succeeded in fitting a VBGE for hake!\n\nNow, $b=2/3$ stems originally from the \"*surface rule*\" [@rubnerGesetzeEnergieverbrauchsBei1902], basically stating that since a (body) surface scales with a 2/3 power to the volume, and heat loss is proportional to the body surface, and because each each calorie must be replaced to maintain a fixed temperature (37°), \"production\" is proportional to $W^{2/3}$. This doesn't apply to ectotherms, such as fish, and in those cases the 2/3 exponent is argued to be close to the exponent of standard metabolic rate (even though we know now it's closer to 0.8 [e.g., @clarkeScalingMetabolicRate1999; @jerdeStrongEvidenceIntraspecific2019; @lindmarkOptimumGrowthTemperature2022]. Or, as Pauly suggests in his Gill-Oxygen Limitation Theory (GOLT): the exponent $y$ reflects the scaling of gill surface area in relation to weight, such that as ectotherms growh in size, they increasingly struggle to meet oxygen demands which scale in proportion to mass.\n\nOk, ok, enough background. The point is this: sometimes you want the parameters $a$ and $b$ rather than $W_\\infty$, $t_0$, and $K_{sp}$. You can convert between them though; $K_{sp}=b/3$ and $W_\\infty=(a/b)^{1/(1–y)}$ [@essingtonBertalanffyGrowthFunction2001]. But note! This is only legal when you make the assumptions above! And you may not always want that. Maybe you want to estimate $a$ and $b$ from data instead. And why not $y$ and $z$ when you are at it? Maybe you don't buy the arguments for the mechanistic basis of these parameters and want to take an empirical approach to fitting the Pütter model. Maybe you want to extend or modify the Pütter model, as in e.g., Marshall & White [-@marshallHaveWeOutgrown2019] or Thunell et al., [-@thunellOptimalEnergyAllocation]. And lastly, you may want the full uncertainty on $a$ and $b$ directly, e.g., via a posterior distribution.\n\nBelow I show how we can fit the Pütter ODE model to data using [brms](https://paul-buerkner.github.io/brms/). As in the last post, we will use the perch (*Perca fluviatilis*) data set containing back-calculated length-at-age. First we'll load, filter, and plot the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RCurl)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(viridis)\nlibrary(brms)\nlibrary(parallel)\nlibrary(truncnorm)\nlibrary(tidybayes)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(patchwork)\nlibrary(ggplot2); theme_set(theme_light())\n\n# Read, crop and clean data\nd <- readr::read_delim(\"https://raw.githubusercontent.com/maxlindmark/warm-life-history/master/data/cleaned/size_at_age_BT_FM_1970-2004.csv\", delim = \";\") %>% \n  mutate(length = length/10,\n         ID = paste(area, ID, sep = \"_\")) %>% \n  group_by(ID) %>% \n  mutate(n = n()) %>%\n  ungroup() %>% \n  filter(area == \"FM\" & catch_age == 9 & !gear == 32 & n <= catch_age) %>% \n  dplyr::select(length, catch_age, age, ID) %>% \n  filter(ID %in% head(unique(ID), 100)) %>% # filter 100 individuals\n  rename(t = age) %>% \n  mutate(w = 0.01*length^3)\n\nggplot(d, aes(t, w, color = ID)) + \n  geom_jitter(height = 0) + \n  geom_line(alpha = 0.4) + \n  guides(color = \"none\") + \n  scale_color_viridis(option = \"E\", discrete = TRUE) + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  ggtitle(\"Back-calculated weight at age grouped by individual\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# For fitting\nnCores <- detectCores()\noptions(mc.cores = nCores)\n```\n:::\n\n\nAs can be seen, we have multiple observations per individual, and they either fast or slow growers. This will cause some unwanted residuals patterns unless accounted for, so we will allow the Pütter parameters to vary by individual (`ID` as random effect). Below I define the model. I'm using the simple exponential decay model in [this](https://discourse.mc-stan.org/t/new-cmdstan-ode-solvers-and-brms/24173) Stan forum post as a template for aunivariate ODE, but I've also read up on the Lotka-Volterra competition model in brms from [Mage's blog](https://www.magesblog.com/post/2021-02-08-fitting-multivariate-ode-models-with-brms/) and [Solomon Kurz's brms-translation of Statistical Rethinking](https://bookdown.org/content/4857/generalized-linear-madness.html). For speed (it still takes a solid 8h to fit this model!) I use only the first 100 individuals in the data set (there are 12786!!!), and I only estimate $a$ and $b$, while setting $y=2/3$ and $z=1$. In theory, one could add nested random effects (ID within cohorts for instance), or let the standard deviation vary with time.\n\n## A Pütter ODE model in brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using this exponential decay model as a template:\n# https://discourse.mc-stan.org/t/new-cmdstan-ode-solvers-and-brms/24173\n\n# Simple vbge model\nvb_model <- \"\n  real[] ode_vb(real t, //time\n  real [] w,         // the rates\n  real [] theta,     // the parameters\n  real [] x_r,       // data constant (not used)\n  int[] x_i){        // data constant (not used)\n  real dwdt[1];      // dimension of ODEs\n\n  dwdt[1] = theta[1]*w[1]^(0.67) - theta[2]*w[1]^1; // growth ODE\n\n  return dwdt;       // returns a 3-dimensional array     \n\n  }\n\n// this is the function call from brms, integration of ODEs:\nreal vb_ode(real t, real weight0, real a, real b) {\n  real w0[1]; //one initial value\n  real theta[2]; \n  real w[1,1]; //ODE solution\n  w0[1] = weight0; //initial values\n  theta[1] = a; \n  theta[2] = b; \n\n  w = integrate_ode_rk45(ode_vb,\n                         w0,\n                         0,\n                         rep_array(t, 1),\n                         theta,\n                         rep_array(0.0,0),\n                         rep_array(1,1),\n                         0.00001,0.00001,100);\n// Return relevant values\n    return(w[1,1]);\n}\n\n\"\n\nvb_formula <- bf(w ~ vb_ode(t, weight0, a, b),\n                 weight0 ~ 1,\n                 a ~ 1 + (1|ID),\n                 b ~ 1 + (1|ID),\n                 nl = TRUE)\n\n# These priors could be discussed...\nvb_priors <- c(prior(normal(0.00001, 1), nlpar = weight0, lb = 0.00001),\n               prior(normal(1, 1), nlpar = a, lb = 0.00001),\n               prior(normal(0.5, 1), nlpar = b, lb = 0.00001),\n               prior(cauchy(0, 10), class = sigma))\n```\n:::\n\n\nNow that all functions have been defined, we can feed that into the `brm` function. I use a Lognormal distribution to ensure we don't predict negative weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# startTime <- Sys.time()\nvb_fit <- brm(data = d,\n              family = lognormal(),\n              formula = vb_formula,\n              prior = vb_priors,\n              init = 0,\n              iter = 4000,\n              chains = 3,\n              cores = 3,\n              backend = \"cmdstanr\",\n              control = list(adapt_delta = 0.9),\n              stanvars = stanvar(scode = vb_model, block = \"functions\"),\n              sample_prior = \"yes\")\n# endTime <- Sys.time()\n# endTime - startTime\n# \n# saveRDS(vb_fit, \"posts/vbge_ode/vb_fit.rds\")\n```\n:::\n\n\n\n\nWe can now check our model summary and plot it to inspect posteriors and chain convergence:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the model!\nsummary(vb_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: w ~ vb_ode(t, weight0, a, b) \n         weight0 ~ 1\n         a ~ 1 + (1 | ID)\n         b ~ 1 + (1 | ID)\n   Data: d (Number of observations: 900) \n  Draws: 3 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 6000\n\nGroup-Level Effects: \n~ID (Number of levels: 100) \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(a_Intercept)     0.06      0.01     0.05     0.07 1.00      897     1736\nsd(b_Intercept)     0.01      0.01     0.00     0.02 1.01      281      463\n\nPopulation-Level Effects: \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nweight0_Intercept     0.41      0.03     0.35     0.47 1.00     2691     3823\na_Intercept           2.35      0.05     2.25     2.44 1.00     2497     3598\nb_Intercept           1.33      0.03     1.27     1.39 1.00     2672     3635\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.30      0.01     0.29     0.32 1.00     6809     4077\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code}\nplot(vb_fit, N=6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nLooks fine! Let's do some data wrangling and plot prior vs posterior (note we can do this because we set `sample_prior = \"yes\"`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- brms::as_draws_df(vb_fit) %>%\n  dplyr::select(b_weight0_Intercept, b_a_Intercept, b_b_Intercept, \n                prior_b_weight0, prior_b_a, prior_b_b) %>%\n  pivot_longer(cols = everything()) %>%\n  mutate(group = ifelse(grepl(\"prior\", name), \"Prior\", \"Posterior\")) %>% \n  mutate(Parameter = str_remove(name, pattern = \"_Intercept\"),\n         Parameter = str_remove(Parameter, pattern = \"prior_\"))\n\nhead(draws)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  name                value group     Parameter\n  <chr>               <dbl> <chr>     <chr>    \n1 b_weight0_Intercept 0.458 Posterior b_weight0\n2 b_a_Intercept       2.30  Posterior b_a      \n3 b_b_Intercept       1.30  Posterior b_b      \n4 prior_b_weight0     0.255 Prior     b_weight0\n5 prior_b_a           1.77  Prior     b_a      \n6 prior_b_b           0.517 Prior     b_b      \n```\n:::\n\n```{.r .cell-code}\n# Plot prior vs posteriors\nggplot(draws) +\n  geom_density(aes(x = value, fill = group), alpha = 0.5, color = NA) +\n  facet_wrap(~Parameter, scales = \"free\") + \n  scale_fill_brewer(palette = \"Set1\", name = \"\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = c(0.85, 0.25)) + \n  labs(x = \"Value\", y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNow let's predict from the model onto a new data (just a dataframe with time steps, representing year) and add uncertainty bands.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd <- data.frame(t = 1:max(d$t))\n\n# We now need to expose the functions to R\nexpose_functions(vb_fit, vectorize = TRUE)\n\npred <- predicted_draws(vb_fit, newdata = nd, ndraws = 1000, re_formula = NA)\n\nggplot(pred, aes(x = as.factor(t), y = w)) +\n  stat_lineribbon(aes(y = .prediction), .width = c(.99, .95, .8, .5), alpha = 0.5) +\n  geom_jitter(data = d, aes(as.factor(t), w), height = 0, alpha = 0.3) +\n  labs(y = \"Weight (g)\", x = \"Time (years)\") +\n  scale_fill_brewer(palette = \"Reds\", name = \"\") + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  ggtitle(\"Global prediction of weight-at-age\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot the model-1.png){width=672}\n:::\n:::\n\n\nWe can also explore the predictions for some of the individuals (first 20 here), and relate that to data and the global prediction. The model seems to have a reasonable global prediction and at the same time a good fit to the individual fish.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnd2 <- data.frame(expand.grid(t = 1:max(d$t),\n                              ID = head(unique(d$ID), 20))) # Use 16 IDs for easier plotting\n\npred2 <- predicted_draws(vb_fit, newdata = nd2, ndraws = 1000)\n\nggplot(pred2, aes(x = as.factor(t), y = w)) +\n  facet_wrap(~ID) +\n  stat_lineribbon(aes(y = .prediction, color = \"ID-prediction\"), .width = c(.99), alpha = 0.8, size = 0.5, fill = \"grey90\") +\n  geom_point(data = d %>% filter(ID %in% head(unique(d$ID), 20)), aes(t, w, color = \"Data\"), size = 0.6) +\n  geom_line(data = pred %>% group_by(t) %>% summarise(median_pred = median(.prediction)),\n            aes(t, median_pred, color = \"Global prediction\"), inherit.aes = FALSE) +\n  scale_color_brewer(palette = \"Set1\", name = \"\") +\n  labs(y = \"Weight (g)\", x = \"Time (years)\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"bottom\") + \n  guides(color = guide_legend(override.aes = list(fill = c(NA, NA, \"grey80\")))) +\n  ggtitle(\"Individual-level and global prediction vs. data\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot individual random effects-1.png){width=672}\n:::\n:::\n\n\nWhat is the $W_\\infty$? We can visualize that by plotting the rates of anabolism and catabolism, and see where they intersect (i.e., where $dW/dt=0$):\n\n\n::: {.cell}\n\n```{.r .cell-code}\npars <- summary(vb_fit)$fixed %>% \n  rownames_to_column() %>% \n  ungroup() %>% \n  mutate(par = str_remove(rowname, pattern = \"_Intercept\")) %>% \n  dplyr::select(par, Estimate) %>% \n  pivot_wider(names_from = par, values_from = Estimate)\n\ndata.frame(mass = log(seq(from = 3, to = 400, by = 0.1))) %>% \n  mutate(Anabolism = pars$a*mass^(2/3),\n         Catabolism = pars$b*mass^1) %>% \n  pivot_longer(c(Anabolism, Catabolism), names_to = \"Rate\") %>% \n  ggplot(aes(exp(mass), value, color = Rate)) + \n  geom_line() + \n  labs(x = \"Mass\", y = \"Rate per year\") + \n  scale_color_brewer(palette = \"Set1\", name = \"\") +\n  ggtitle(\"Anabolism vs. catabolism\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = c(0.9, 0.1),\n        legend.background = element_rect(fill = NA))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nTo wrap it up and link back to the motivation for this model. In my case, I want the parameters $a$ and $b$ with uncertainty and without having to make assumptions about isometric growth or certain scaling exponents. Because with $a$ and $b$ and the Pütter ODE (rhyme not intended), I can for loop change in weight for any time step (e.g., daily if I divide by 365), and I can add things like seasonality (temperature). Here I exemplify the for loop approach and compare it to data, the global predictions and the distribution of $W_\\infty$ that we get from this relation: $W_\\infty=(H/k)^{1/(1–d)}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_age <- 10\nweight_df <- data.frame(weight = c(pars$weight0, rep(NA, max_age)),\n                        age = c(0:max_age),\n                        anabolism = rep(NA, max_age + 1),\n                        catabolism = rep(NA, max_age + 1),\n                        dw = rep(NA, max_age + 1))\n\nfor(i in 2:nrow(weight_df)){\n  \n  anabolism <- pars$a*weight_df[i-1, \"weight\"]^(2/3)\n  catabolism <- pars$b*weight_df[i-1, \"weight\"]^(1)\n  \n  dw <- anabolism - catabolism\n  \n  weight_df[i, \"anabolism\"] <- anabolism\n  weight_df[i, \"catabolism\"] <- catabolism\n  weight_df[i, \"dw\"] <- dw\n  weight_df[i, \"weight\"] <- weight_df[i-1, \"weight\"] + dw\n  \n}\n\n# Get new prediction with a longer time span to get the asymptote\nnd <- data.frame(t = 1:max_age)\n\n# Predict from the log-normal model\npred <- predicted_draws(vb_fit, newdata = nd, ndraws = 1000, re_formula = NA)\n\n# Let's plot the data, the model prediction and the \"manual\" prediction using the for loops and our parameter estimates.\np1 <- ggplot(d, aes(t, w)) + \n  stat_lineribbon(data = pred, aes(t, .prediction, color = \"Global prediction\"), inherit.aes = FALSE, .width = c(.99, .95, .8, .5), alpha = 0.5) +\n  geom_jitter(height = 0, alpha = 0.5) +  \n  geom_line(data = weight_df, aes(age, exp(weight), color = \"Manual prediction\"), size = 1.1) +\n  scale_color_brewer(palette = \"Set1\", name = \"\") +\n  scale_fill_brewer(palette = \"Reds\", name = \"\") + \n  coord_cartesian(ylim = c(0, 700)) + \n  labs(y = \"Weight (g)\", x = \"Time (years)\") +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = c(0.15, 0.95),\n        legend.background = element_rect(fill = NA)) +\n  guides(color = guide_legend(override.aes = list(fill = NA)),\n         fill = \"none\")\n\n# Get w_inf not as a fixed value but as a distribution given the posterior for a and b\np2 <- post_draws <- brms::as_draws_df(vb_fit) %>% \n  mutate(w_inf = exp((b_a_Intercept/b_b_Intercept)^(1/(1-(2/3))))) %>% \n  filter(w_inf < 700) %>% \n  ggplot(aes(w_inf)) + \n  geom_density() +\n  labs(x = \"\", y = \"Density\") +\n  coord_flip(xlim = c(0, 700)) +\n  scale_y_continuous(breaks = c(0, 0.05)) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  NULL\n\np1 + p2 + plot_layout(widths = c(5, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/manual calculations-1.png){width=672}\n:::\n:::\n\n\n::: {#refs}\n:::\n\n##### Further reading on von Bertalanffy and GOLT\n\n###### Global Change Biology\n\nLefevre, S., McKenzie, D. J., and Nilsson, G. E. 2017. Models projecting the fate of fish populations under climate change need to be based on valid physiological mechanisms. Global Change Biology, 23: 3449--3459.\n\nPauly, D., and Cheung, W. W. L. 2018. Sound physiological knowledge and principles in modeling shrinking of fishes under climate change. Global Change Biology, 24: e15--e26.\n\nLefevre, S., McKenzie, D. J., and Nilsson, G. E. 2018. In modelling effects of global warming, invalid assumptions lead to unrealistic projections. Global Change Biology, 24: 553--556.\n\nPauly, D., and Cheung, W. W. L. 2018. On confusing cause and effect in the oxygen limitation of fish. Global Change Biology, 24: e743--e744.\n\n###### Trends in Ecology and Evolution\n\nMarshall, D. J., and White, C. R. 2019. Have we outgrown the existing models of growth? Trends in Ecology & Evolution, 34: 102--111.\n\nPauly, D. 2019. Female Fish Grow Bigger -- Let's Deal with It. Trends in Ecology & Evolution, 34, 3\n\nMarshall, D. J., and White, C. R. 2019. Aquatic life history trajectories are shaped by selection, not oxygen limitation. Trends in Ecology & Evolution, 34: 182--184.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}